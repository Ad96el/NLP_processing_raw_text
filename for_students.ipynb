{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0-final"
        },
        "orig_nbformat": 2,
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.0 64-bit ('venv')",
            "metadata": {
                "interpreter": {
                    "hash": "38c80e9a7e900bc721b50b6770c57dc44f98843d02742c3cc731578aff1b0225"
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "source": [
                "<center> <h1> Natural Language Processing with Python </h1> </center>\n",
                "<center> <h2> Processing Raw Text  </h2> </center> \n",
                "<center> <img height=\"300\" src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Natural-Language-Processing-with-Python.jpg\"> </ center>"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Goals\n",
                "\n",
                "The goal of this chapter is to answer the questions: \n",
                "\n",
                "\n",
                "- How can we write programs to access texts?\n",
                "\n",
                "- How can we process those text?\n",
                "\n",
                "- How can we write programs to produce formatted output and save those?\n",
                " "
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "_______"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Some Basics in Python\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "example = \"This is an example string!\"\n",
                "\n",
                "\"\"\"\n",
                "We can index the values of a string, by using [index].\n",
                "Remind: in python negative index values are also possible!\n",
                "\"\"\"\n",
                "\n",
                "print(example[0])\n",
                "print(example[1])\n",
                "\n",
                "\"\"\"\n",
                "The concatenation of two strings can be realised by \"+\".\n",
                "\"\"\"\n",
                "\n",
                "example2 = \"This is an other example of a string!\"\n",
                "print(example + example2)\n",
                "\n",
                "\"\"\"\n",
                "We can also access to substrings by using square brackets. \n",
                "The first value is determining the start of the substring. The Second value the end and the last value the step length: [start:end:step_lenght]\n",
                "\"\"\"\n",
                "\n",
                "print(example[:10])\n",
                "print(example[10:])\n",
                "print(example[::2])"
            ]
        },
        {
            "source": [
                "<h3 style =\"color: red\" > Tasks: <h3 />\n",
                "\n",
                "- Define a string s = 'The Godfther'. Write a statement that changes this to \"The Godfather\". You can only use concatenation and slicing.  \n",
                "\n",
                "- What will happen if we will access on the 13rd element of the string s? Why? \n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here: "
            ]
        },
        {
            "source": [
                "________"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Accessing Text\n",
                "\n",
                "In this section we will discuss three methods for acessing text. \n",
                "\n",
                "- Reading local Files (.txt, PDF) \n",
                "- Web"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "#### Acessing Text from local file system"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_text_from_txt(path):\n",
                "    f = open(path)\n",
                "    raw = f.read() \n",
                "    return raw\n",
                "\n",
                "example1 = get_text_from_txt(\"./example.txt\")\n",
                "print(example1[:100])"
            ]
        },
        {
            "source": [
                "#### Acessing Text from binary Formats\n",
                "\n",
                "For more information, please have a look on <a href=\"https://github.com/jsvine/pdfplumber\">PDFplumber</a> documentation."
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pdfplumber\n",
                "\n",
                "def get_text_from_pdf(path):\n",
                "    with pdfplumber.open(path) as pdf:\n",
                "        first_page = pdf.pages[0]\n",
                "        print(len(pdf.pages))\n",
                "        print(first_page.extract_text()[:100])\n",
                "\n",
                "get_text_from_pdf(\"./example.pdf\")"
            ]
        },
        {
            "source": [
                "#### Acessing Text from web\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ssl\n",
                "try:\n",
                "    _create_unverified_https_context = ssl._create_unverified_context\n",
                "except AttributeError:\n",
                "    pass\n",
                "else:\n",
                "    ssl._create_default_https_context = _create_unverified_https_context\n",
                "    \n",
                "from urllib.request import urlopen\n",
                "\n",
                "def get_text_from_url(url):\n",
                "    raw = urlopen(url).read() \n",
                "    return raw\n",
                "\n",
                "url = \"https://en.wikipedia.org/wiki/The_Godfather\"\n",
                "raw = get_text_from_url(url)\n",
                "\n",
                "print(type(raw))\n",
                "print(len(raw))\n",
                "print(\"Content of the Website: \\n\" ,raw[:100])"
            ]
        },
        {
            "source": [
                "By access text from the Web, we will always receive all meta tags from the HTML protocol.\n",
                "\n",
                "<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Beautiful Soup</a>  is a providing helper function for pulling the text out of the tags."
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from bs4 import BeautifulSoup\n",
                "\n",
                "soup = BeautifulSoup(raw)\n",
                "\n",
                "soup.get_text()[:100]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = soup.find(id=\"firstHeading\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = soup.find_all(\"p\") \n",
                "print(len(text))\n",
                "print(text[1].get_text())"
            ]
        },
        {
            "source": [
                "<h3 style =\"color: red\" > Tasks: <h3 />\n",
                "\n",
                "- Write a code to get the the current teperature of Berlin. \n",
                "\n",
                "(Hint: use this Link: https://www.bbc.com/weather/2950159. The important area can be identified by the class=\"wr-value--temperature--c\")\n",
                "\n",
                " "
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "# your code here "
            ],
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "source": [
                "_____________________"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Processing Text\n",
                "\n",
                "In this section we cover:\n",
                "\n",
                "- how we can deal with different Languages\n",
                "\n",
                "- the use of regular expressions for stemming \n",
                "\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "### Text Processing with Unicode\n",
                "\n",
                "Unicode supports over a million characters.\n",
                "\n",
                "written form: \\XXXX"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import codecs \n",
                "import unicodedata\n",
                "\n",
                "line = codecs.open(\"./example2.txt\", encoding=\"utf-8\").readlines()[0]\n",
                "\n",
                "print(line.encode(\"unicode_escape\") )\n",
                "\n",
                "for c in line:\n",
                "    if(ord(c) > 127):\n",
                "        print(c, c.encode(\"unicode_escape\") , ord(c) ,unicodedata.name(c))\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = line.replace('ø' , \"o|\")\n",
                "line = line.replace(\"å\" , \"a_\") \n",
                "print(line.encode(\"GB2312\"))"
            ]
        },
        {
            "source": [
                "\n",
                "<img src=\"https://www.nltk.org/images/unicode.png\" width=\"90%\" height=\"400\"/>"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "__________________"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "### Regular Expressions\n",
                "\n",
                "In NLP, there are a lot of tasks involving pattern matching. Regular expressions give us a powerful and flexible method.\n",
                "\n",
                "\n",
                "|Operator |Behavior     |\n",
                "|-------------|-------------|\n",
                "| .     | Wildcard, matches any character|\n",
                "| ^abc      | Matches some pattern abc at the start of a string    | \n",
                "| abc$ | Matches some pattern abc at the end of a string     | \n",
                "| \\[abc\\]      | Matches one of a set of characters|\n",
                "| \\[A-Z\\]      | Matches one of a range of characters  | \n",
                "| ed\\|es | Matches one of the specified strings (disjunction)  | \n",
                "| *      | Zero or more of previous item, e.g. a\\*, \\[a-z\\]\\* (also known as Kleene Closure)|\n",
                "| +      | One or more of previous item, e.g. a+, \\[a-z\\]+    | \n",
                "| ? | Zero or one of the previous item (i.e. optional), e.g. a?, \\[a-z\\]?   | \n",
                "| {n}      | Exactly n repeats where n is a non-negative integer|\n",
                "| {n,}      | At least n repeats | \n",
                "| {,n} | No more than n repeats   | \n",
                "| {m,n}      | At least m and no more than n repeats|"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "import re\n",
                "\n",
                "res = re.search(r\"ed\",\"abaiedsse\")\n",
                "\n",
                "print(res)\n",
                "print(res.start())\n",
                "print(res.end())\n",
                "print(res.string) "
            ],
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "\n",
                "wordlist_en = [w.lower() for w in nltk.corpus.words.words(\"en\")]\n",
                "\n",
                "list_ =  [w for w in wordlist_en if re.search(r\"^ho\" , w)]\n",
                "print(len(list_))\n",
                "list_[:10]"
            ]
        },
        {
            "source": [
                "<h3 style =\"color: red\" > Tasks: <h3 />\n",
                "\n",
                "- extract all numbers with a length of 4 from wsj. \n",
                "\n",
                "- since the numbers are valid year numbers, what is the ratio of numbers from the 80s."
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wordlist_wsj = nltk.corpus.treebank.words()\n",
                "# your code here"
            ]
        },
        {
            "source": [
                "### more usefull functions"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word = 'supercalifragilisticexpialidociou'\n",
                "a = re.findall(r'[aeiou]', word)\n",
                "a[:5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word = 'Today is the 25th of July. It is a beautiful day. My mom said: \"dont run so fast!\". \\'Why?\\' did I ask.'\n",
                "a = re.sub(r'[0-9.!?\"*#\\']', \"\", word)\n",
                "a"
            ]
        },
        {
            "source": [
                "### Stemming"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stem(word):\n",
                "    stem, suff = re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$' , word)[0]\n",
                "    return stem, suff"
            ]
        },
        {
            "source": [
                "# Text Normalization \n",
                "\n",
                "Text normalization is the process of transforming of the text from one form to another form, having the relevant context is preservered."
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Text Stemming\n",
                "\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ssl\n",
                "try:\n",
                "    _create_unverified_https_context = ssl._create_unverified_context\n",
                "except AttributeError:\n",
                "    pass\n",
                "else:\n",
                "    ssl._create_default_https_context = _create_unverified_https_context\n",
                "\n",
                "import nltk\n",
                "\n",
                "raw_text = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
                "... is no basis for a system of government.  Supreme executive power derives from\n",
                "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
                "tokens = nltk.word_tokenize(raw_text)\n",
                "print(tokens)"
            ]
        },
        {
            "source": [
                "### Porter Stemmer"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "porter = nltk.PorterStemmer()\n",
                "p_tokens = [porter.stem(token) for token in tokens]\n",
                "print(p_tokens)"
            ],
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "source": [
                "### Lancaster Stemmer"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lancaster = nltk.LancasterStemmer()\n",
                "l_tokens = [lancaster.stem(token) for token in tokens]\n",
                "print(l_tokens)"
            ]
        },
        {
            "source": [
                "## Text Lemmatization"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lem = nltk.WordNetLemmatizer()\n",
                "lem_tokens = [lem.lemmatize(token) for token in tokens]\n",
                "print(lem_tokens)"
            ]
        },
        {
            "source": [
                "Task:\n",
                "- Given the text:\n",
                "\"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
                "\n",
                "1. Stem the words using both the Porter and Lancaster stemmers\n",
                "2. Find the difference between those two result sets, by listing the stemmed words which are not common between those two results."
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here"
            ]
        },
        {
            "source": [
                "# Text Tokenization"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"\"\"'Hi there. I'M a Phillip,' introduced the teacher himself to children, (not in a very hopeful tone\n",
                "... though), 'We won't have any boring stuff on my classes AT ALL. I hope you would all do it very\n",
                "... well without--Maybe it's always better to take notes of my life-changing classes,'...\"\"\""
            ]
        },
        {
            "source": [
                "Goal: get words of the text"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "words = re.split(r' ', text)\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = re.split(r'[ \\t\\n]+', text)\n",
                "words = re.split(r'\\s+', text)\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = re.split(r'[^a-zA-Z0-9_]', text)\n",
                "words = re.split(r'\\W+', text)\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = re.findall(r'[a-zA-Z0-9_]+', text) \n",
                "words = re.findall(r'\\w+', text)\n",
                "print(words)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = re.findall(r'\\w+|\\S\\w*', text)\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", text)\n",
                "print(words)"
            ]
        },
        {
            "source": [
                "## Additinal Regular Expressions\n",
                "\n",
                "|Operator |Behavior     |\n",
                "|-------------|-------------|\n",
                "| \\b | Word boundary (zero width) |\n",
                "| \\d | Any decimal digit (equivalent to \\[0-9\\])    | \n",
                "| \\D | Any non-digit character (equivalent to \\[^0-9\\])     | \n",
                "| \\s | Any whitespace character (equivalent to \\[ \\t\\n\\r\\f\\v\\]) |\n",
                "| \\S | Any non-whitespace character (equivalent to \\[^ \\t\\n\\r\\f\\v\\])  | \n",
                "| \\w | Any alphanumeric character (equivalent to \\[a-zA-Z0-9_\\])  | \n",
                "| \\W | Any non-alphanumeric character (equivalent to \\[^a-zA-Z0-9_\\])|\n",
                "| \\t | The tab character    | \n",
                "| \\n | The newline character   | "
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text ='That U.S.A. poster-print costs $12.40...'\n",
                "pattern = '''(?x)\n",
                "    (?:[A-Z]\\.)+  # abbreviations\n",
                "    |\\w+(?:-\\w+)* # words with optional internal dash\n",
                "    |\\$?\\d+(?:\\.\\d+)?%? # currency and percentages\n",
                "    | \\.\\.\\. # ellipsis\n",
                "    |[][.,;\"'?():-_`] # separate tokens'''\n",
                "words = nltk.regexp_tokenize(text, pattern)\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "expected_result = [\"poster-print\", \"costs\", \"U.S.A\"]\n",
                "difference = set(words).difference(expected_result)\n",
                "print(difference)"
            ]
        },
        {
            "source": [
                "Task:\n",
                "- Given text: \"Sie Yu Chuah smiles when asked how his parents would react to a low test score. “My parents are not that strict but they have high expectations of me,” he says. The cheerful, slightly built 13-year-old is a pupil at Admiralty, it's a government secondary school in the northern suburbs of Singapore that opened in 2002.\"\n",
                "\n",
                "1. Tokenize the text using one of the regexes introduced\n",
                "2. Retrieve the lexicon of the text\n",
                "\n",
                "Note: this basic check could be helpful to check whether the token is a valid word\n",
                "```\n",
                "def isSpecialCharacter(word):\n",
                "    return len(word) == 1 and not word.isalpha()\n",
                "```\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here"
            ]
        },
        {
            "source": [
                "# Text Segmentation"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "source": [
                "## Sentence Segmentation\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nltk.download('brown')\n",
                "avg = len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\n",
                "print(avg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "nltk.download('gutenberg')\n",
                "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
                "sentences = nltk.sent_tokenize(text)\n",
                "print(sentences[79:89])"
            ]
        },
        {
            "source": [
                "## Word Segmentation\n"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
                "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
                "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def segment(text, segmentation):\n",
                "    words = []\n",
                "    last = 0\n",
                "    for i in range(len(segmentation)):\n",
                "        if segmentation[i] == '1':\n",
                "            words.append(text[last:i+1])\n",
                "            last = i+1\n",
                "    words.append(text[last:])\n",
                "    return words\n",
                "\n",
                "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
                "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
                "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
                "print(segment(text, seg1))\n",
                "print(segment(text, seg2))"
            ]
        },
        {
            "source": [
                "<img src=\"segmentation.png\" />"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(text, segmentation):\n",
                "    words = segment(text, segmentation)\n",
                "    text_size = len(words)\n",
                "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
                "    return text_size + lexicon_size\n",
                "\n",
                "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
                "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
                "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
                "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
                "print(evaluate(text, seg1))\n",
                "print(evaluate(text, seg2))\n",
                "print(evaluate(text, seg3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from random import randint\n",
                "\n",
                "def flip(segs, pos):\n",
                "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
                "\n",
                "def flip_n(segs, n):\n",
                "    for i in range(n):\n",
                "        segs = flip(segs, randint(0, len(segs)-1))\n",
                "    return segs\n",
                "\n",
                "def anneal(text, segs, iterations, cooling_rate):\n",
                "    temperature = float(len(segs))\n",
                "    while temperature > 0.5:\n",
                "        best_segs, best = segs, evaluate(text, segs)\n",
                "        for i in range(iterations):\n",
                "            guess = flip_n(segs, round(temperature))\n",
                "            score = evaluate(text, guess)\n",
                "            if score < best:\n",
                "                best, best_segs = score, guess\n",
                "        score, segs = best, best_segs\n",
                "        temperature = temperature / cooling_rate\n",
                "        print(evaluate(text, segs), segment(text, segs))\n",
                "    print()\n",
                "    return segs\n",
                "\n",
                "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
                "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
                "anneal(text, seg1, 5000, 1.2)"
            ]
        },
        {
            "source": [
                "# Formatting: From Lists to Strings"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
                "print(' '.join(silly))\n",
                "print(';'.join(silly))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
                "for word in sorted(fdist):\n",
                "    print(word, '->', fdist[word], end='; ')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = '{}->{};'.format ('cat', 3)\n",
                "print(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = 'from {1} to {0}'.format('A', 'B')\n",
                "print(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('{:<6}' .format(41))\n",
                "print('{:{width}}'.format('Monty Python', width=195))\n",
                "print('{:.4f}'.format(3.148381201293189))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_file = open('output.txt', 'w')\n",
                "words = ['one', 'two', 'three']\n",
                "for word in words:\n",
                "    print(word, file=output_file)"
            ]
        },
        {
            "source": [
                "THANK YOU FOR ATTENTION!\n",
                "\n",
                "<img src=\"https://15f76u3xxy662wdat72j3l53-wpengine.netdna-ssl.com/wp-content/uploads/2016/12/Christmas-card_not-raw-FINAL-1.gif\" width=\"90%\"/>"
            ],
            "cell_type": "markdown",
            "metadata": {}
        }
    ]
}